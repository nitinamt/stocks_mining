{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "100ec240-b24b-468a-9825-5071a28f1f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import requests\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import random\n",
    "\n",
    "from mycongif import ConfigManager\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RobustStockNewsEventsFetcher:\n",
    "    def __init__(self, db_path: str = \"db/stock_news.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.session = requests.Session()\n",
    "        self.apikeys = ConfigManager(\"private/apikeys.ini\")\n",
    "        \n",
    "        # Rotate user agents to avoid detection\n",
    "        self.user_agents = [\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0'\n",
    "        ]\n",
    "        \n",
    "        self.init_database()\n",
    "        \n",
    "        # Updated news sources with better strategies\n",
    "        self.news_sources = {\n",
    "            'yahoo_finance_api': {\n",
    "                'function': self._fetch_yahoo_news_api,\n",
    "                'description': 'Yahoo Finance News API'\n",
    "            },\n",
    "            'alpha_vantage_news': {\n",
    "                'function': self._fetch_alpha_vantage_news,\n",
    "                'description': 'Alpha Vantage News API'\n",
    "            },\n",
    "            'newsapi': {\n",
    "                'function': self._fetch_newsapi,\n",
    "                'description': 'NewsAPI.org'\n",
    "            },\n",
    "            'financial_modeling_prep': {\n",
    "                'function': self._fetch_fmp_news,\n",
    "                'description': 'Financial Modeling Prep'\n",
    "            },\n",
    "            'polygon_news': {\n",
    "                'function': self._fetch_polygon_news,\n",
    "                'description': 'Polygon.io News'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def init_database(self):\n",
    "        \"\"\"Initialize SQLite database with required tables\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create news table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS news_articles (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                ticker TEXT NOT NULL,\n",
    "                title TEXT NOT NULL,\n",
    "                url TEXT NOT NULL,\n",
    "                content TEXT,\n",
    "                summary TEXT,\n",
    "                publisher TEXT,\n",
    "                author TEXT,\n",
    "                publish_date DATETIME,\n",
    "                scraped_date DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "                source_site TEXT,\n",
    "                content_hash TEXT UNIQUE,\n",
    "                sentiment_score REAL,\n",
    "                UNIQUE(url, ticker)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Create events table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS stock_events (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                ticker TEXT NOT NULL,\n",
    "                event_date DATE,\n",
    "                event_type TEXT,\n",
    "                event_description TEXT,\n",
    "                event_value REAL,\n",
    "                scraped_date DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "                UNIQUE(ticker, event_date, event_type, event_description)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Create price movements table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS price_movements (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                ticker TEXT NOT NULL,\n",
    "                movement_date DATE,\n",
    "                price_change_percent REAL,\n",
    "                close_price REAL,\n",
    "                volume INTEGER,\n",
    "                movement_type TEXT,\n",
    "                scraped_date DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "                UNIQUE(ticker, movement_date)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Create indexes for better performance\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_ticker_date ON news_articles(ticker, publish_date)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_events_ticker_date ON stock_events(ticker, event_date)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_movements_ticker_date ON price_movements(ticker, movement_date)')\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def get_content_hash(self, content: str) -> str:\n",
    "        \"\"\"Generate MD5 hash of content to detect duplicates\"\"\"\n",
    "        return hashlib.md5(content.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    def is_news_exists(self, url: str, ticker: str, content_hash: str) -> bool:\n",
    "        \"\"\"Check if news article already exists in database\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('''\n",
    "            SELECT id FROM news_articles \n",
    "            WHERE (url = ? AND ticker = ?) OR content_hash = ?\n",
    "        ''', (url, ticker, content_hash))\n",
    "        \n",
    "        exists = cursor.fetchone() is not None\n",
    "        conn.close()\n",
    "        return exists\n",
    "    \n",
    "    def save_news_to_db(self, news_data: Dict, ticker: str):\n",
    "        \"\"\"Save news article to database if it doesn't exist\"\"\"\n",
    "        content_hash = self.get_content_hash(news_data.get('content', '') + news_data.get('title', ''))\n",
    "        \n",
    "        if self.is_news_exists(news_data.get('url', ''), ticker, content_hash):\n",
    "            logger.info(f\"News already exists: {news_data.get('title', 'Unknown')[:50]}...\")\n",
    "            return False\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO news_articles \n",
    "                (ticker, title, url, content, summary, publisher, author, publish_date, source_site, content_hash)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                ticker,\n",
    "                news_data.get('title', ''),\n",
    "                news_data.get('url', ''),\n",
    "                news_data.get('content', ''),\n",
    "                news_data.get('summary', ''),\n",
    "                news_data.get('publisher', ''),\n",
    "                news_data.get('author', ''),\n",
    "                news_data.get('publish_date'),\n",
    "                news_data.get('source_site', ''),\n",
    "                content_hash\n",
    "            ))\n",
    "            conn.commit()\n",
    "            logger.info(f\"Saved new news: {news_data.get('title', 'Unknown')[:50]}...\")\n",
    "            return True\n",
    "        except sqlite3.IntegrityError:\n",
    "            logger.info(f\"Duplicate news detected: {news_data.get('title', 'Unknown')[:50]}...\")\n",
    "            return False\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def _get_random_headers(self):\n",
    "        \"\"\"Get random headers to avoid detection\"\"\"\n",
    "        return {\n",
    "            'User-Agent': random.choice(self.user_agents),\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        }\n",
    "    \n",
    "    def _fetch_yahoo_news_api(self, ticker: str) -> List[Dict]:\n",
    "        \"\"\"Fetch news using Yahoo Finance's internal API\"\"\"\n",
    "        articles = []\n",
    "        try:\n",
    "            # Try Yahoo's news API endpoint\n",
    "            url = f\"https://query1.finance.yahoo.com/v1/finance/search?q={ticker}\"\n",
    "            headers = self._get_random_headers()\n",
    "            \n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                # Process news if available in response\n",
    "                if 'news' in data:\n",
    "                    for item in data['news'][:10]:\n",
    "                        articles.append({\n",
    "                            'title': item.get('title', ''),\n",
    "                            'url': item.get('link', ''),\n",
    "                            'content': item.get('summary', ''),\n",
    "                            'summary': item.get('summary', '')[:200] + '...' if len(item.get('summary', '')) > 200 else item.get('summary', ''),\n",
    "                            'publisher': item.get('publisher', 'Yahoo Finance'),\n",
    "                            'author': '',\n",
    "                            'publish_date': self._convert_timestamp(item.get('providerPublishTime')),\n",
    "                            'source_site': 'yahoo_finance_api'\n",
    "                        })\n",
    "            \n",
    "            # Alternative: Try yfinance news (more reliable)\n",
    "            try:\n",
    "                stock = yf.Ticker(ticker)\n",
    "                news = stock.news\n",
    "                for item in news[:10]:\n",
    "                    articles.append({\n",
    "                        'title': item.get('title', ''),\n",
    "                        'url': item.get('link', ''),\n",
    "                        'content': item.get('summary', ''),\n",
    "                        'summary': item.get('summary', '')[:200] + '...' if len(item.get('summary', '')) > 200 else item.get('summary', ''),\n",
    "                        'publisher': item.get('publisher', 'Yahoo Finance'),\n",
    "                        'author': '',\n",
    "                        'publish_date': self._convert_timestamp(item.get('providerPublishTime')),\n",
    "                        'source_site': 'yfinance_news'\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"yfinance news error: {e}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Yahoo News API error: {e}\")\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    def _fetch_alpha_vantage_news(self, ticker: str) -> List[Dict]:\n",
    "        \"\"\"Fetch news from Alpha Vantage (requires API key)\"\"\"\n",
    "        articles = []\n",
    "        # Note: This requires an API key from Alpha Vantage\n",
    "        # You can get a free one at https://www.alphavantage.co/support/#api-key\n",
    "        \n",
    "        api_key = self.apikeys.get_key(\"alpha_vantage\", \"apikey\") # Replace with your API key\n",
    "        if api_key == \"YOUR_ALPHA_VANTAGE_API_KEY\":\n",
    "            logger.info(\"Alpha Vantage API key not provided, skipping...\")\n",
    "            return articles\n",
    "        \n",
    "        try:\n",
    "            url = f\"https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={ticker}&apikey={api_key}\"\n",
    "            response = requests.get(url, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if 'feed' in data:\n",
    "                    for item in data['feed'][:10]:\n",
    "                        articles.append({\n",
    "                            'title': item.get('title', ''),\n",
    "                            'url': item.get('url', ''),\n",
    "                            'content': item.get('summary', ''),\n",
    "                            'summary': item.get('summary', '')[:200] + '...' if len(item.get('summary', '')) > 200 else item.get('summary', ''),\n",
    "                            'publisher': item.get('source', 'Alpha Vantage'),\n",
    "                            'author': item.get('authors', [''])[0] if item.get('authors') else '',\n",
    "                            'publish_date': item.get('time_published', ''),\n",
    "                            'source_site': 'alpha_vantage'\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Alpha Vantage error: {e}\")\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    def _fetch_newsapi(self, ticker: str) -> List[Dict]:\n",
    "        \"\"\"Fetch news from NewsAPI.org (requires API key)\"\"\"\n",
    "        articles = []\n",
    "        # Note: This requires an API key from NewsAPI.org\n",
    "        # You can get a free one at https://newsapi.org/\n",
    "        \n",
    "        api_key = self.apikeys.get_key(\"newsapi\", \"apikey\")  # Replace with your API key\n",
    "        if api_key == \"YOUR_NEWSAPI_KEY\":\n",
    "            logger.info(\"NewsAPI key not provided, skipping...\")\n",
    "            return articles\n",
    "        \n",
    "        try:\n",
    "            # Get company name for better search results\n",
    "            stock = yf.Ticker(ticker)\n",
    "            company_name = stock.info.get('longName', ticker)\n",
    "            \n",
    "            url = f\"https://newsapi.org/v2/everything?q={company_name}&apiKey={api_key}&sortBy=publishedAt&pageSize=10\"\n",
    "            response = requests.get(url, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if 'articles' in data:\n",
    "                    for item in data['articles']:\n",
    "                        articles.append({\n",
    "                            'title': item.get('title', ''),\n",
    "                            'url': item.get('url', ''),\n",
    "                            'content': item.get('content', ''),\n",
    "                            'summary': item.get('description', '')[:200] + '...' if len(item.get('description', '')) > 200 else item.get('description', ''),\n",
    "                            'publisher': item.get('source', {}).get('name', 'NewsAPI'),\n",
    "                            'author': item.get('author', ''),\n",
    "                            'publish_date': item.get('publishedAt', ''),\n",
    "                            'source_site': 'newsapi'\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"NewsAPI error: {e}\")\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    def _fetch_fmp_news(self, ticker: str) -> List[Dict]:\n",
    "        \"\"\"Fetch news from Financial Modeling Prep (requires API key)\"\"\"\n",
    "        articles = []\n",
    "        # Note: This requires an API key from Financial Modeling Prep\n",
    "        # You can get a free one at https://financialmodelingprep.com/\n",
    "        \n",
    "        api_key = self.apikeys.get_key(\"fmp\", \"apikey\")  # Replace with your API key\n",
    "        if api_key == \"YOUR_FMP_API_KEY\":\n",
    "            logger.info(\"FMP API key not provided, skipping...\")\n",
    "            return articles\n",
    "        \n",
    "        try:\n",
    "            url = f\"https://financialmodelingprep.com/api/v3/stock_news?tickers={ticker}&limit=10&apikey={api_key}\"\n",
    "            response = requests.get(url, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                for item in data:\n",
    "                    articles.append({\n",
    "                        'title': item.get('title', ''),\n",
    "                        'url': item.get('url', ''),\n",
    "                        'content': item.get('text', ''),\n",
    "                        'summary': item.get('text', '')[:200] + '...' if len(item.get('text', '')) > 200 else item.get('text', ''),\n",
    "                        'publisher': item.get('site', 'FMP'),\n",
    "                        'author': '',\n",
    "                        'publish_date': item.get('publishedDate', ''),\n",
    "                        'source_site': 'fmp'\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"FMP error: {e}\")\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    def _fetch_polygon_news(self, ticker: str) -> List[Dict]:\n",
    "        \"\"\"Fetch news from Polygon.io (requires API key)\"\"\"\n",
    "        articles = []\n",
    "        # Note: This requires an API key from Polygon.io\n",
    "        # You can get a free one at https://polygon.io/\n",
    "        \n",
    "        api_key = self.apikeys.get_key(\"polygon\", \"apikey\")  # Replace with your API key\n",
    "        if api_key == \"YOUR_POLYGON_API_KEY\":\n",
    "            logger.info(\"Polygon API key not provided, skipping...\")\n",
    "            return articles\n",
    "        \n",
    "        try:\n",
    "            url = f\"https://api.polygon.io/v2/reference/news?ticker={ticker}&limit=10&apikey={api_key}\"\n",
    "            response = requests.get(url, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if 'results' in data:\n",
    "                    for item in data['results']:\n",
    "                        articles.append({\n",
    "                            'title': item.get('title', ''),\n",
    "                            'url': item.get('article_url', ''),\n",
    "                            'content': item.get('description', ''),\n",
    "                            'summary': item.get('description', '')[:200] + '...' if len(item.get('description', '')) > 200 else item.get('description', ''),\n",
    "                            'publisher': item.get('publisher', {}).get('name', 'Polygon'),\n",
    "                            'author': item.get('author', ''),\n",
    "                            'publish_date': item.get('published_utc', ''),\n",
    "                            'source_site': 'polygon'\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Polygon error: {e}\")\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    def _convert_timestamp(self, timestamp):\n",
    "        \"\"\"Convert various timestamp formats to standard format\"\"\"\n",
    "        if not timestamp:\n",
    "            return datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        try:\n",
    "            if isinstance(timestamp, (int, float)):\n",
    "                # Unix timestamp\n",
    "                return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')\n",
    "            else:\n",
    "                # String format\n",
    "                return pd.to_datetime(timestamp).strftime('%Y-%m-%d')\n",
    "        except:\n",
    "            return datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    def scrape_news_for_ticker(self, ticker: str, days_back: int = 30) -> int:\n",
    "        \"\"\"Scrape news from multiple sources for a given ticker\"\"\"\n",
    "        total_new_articles = 0\n",
    "        \n",
    "        for source_name, source_config in self.news_sources.items():\n",
    "            logger.info(f\"Fetching from {source_config['description']} for {ticker}...\")\n",
    "            \n",
    "            try:\n",
    "                # Fetch articles using source-specific function\n",
    "                articles = source_config['function'](ticker)\n",
    "                \n",
    "                # Save new articles to database\n",
    "                new_count = 0\n",
    "                for article in articles:\n",
    "                    if self.save_news_to_db(article, ticker):\n",
    "                        new_count += 1\n",
    "                \n",
    "                total_new_articles += new_count\n",
    "                logger.info(f\"Found {len(articles)} articles from {source_name}, {new_count} were new\")\n",
    "                \n",
    "                # Be respectful to APIs\n",
    "                time.sleep(2)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error fetching from {source_name} for {ticker}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return total_new_articles\n",
    "    \n",
    "    def get_stock_data_and_news(self, ticker: str, years_back: int = 10) -> Dict:\n",
    "        \"\"\"Get comprehensive stock data including scraped news\"\"\"\n",
    "        logger.info(f\"Starting comprehensive data collection for {ticker}\")\n",
    "        \n",
    "        # First, scrape fresh news\n",
    "        new_articles_count = self.scrape_news_for_ticker(ticker, days_back=365)\n",
    "        logger.info(f\"Scraped {new_articles_count} new articles for {ticker}\")\n",
    "        \n",
    "        # Get traditional yfinance data\n",
    "        stock_data = self._get_yfinance_data(ticker, years_back)\n",
    "        \n",
    "        # Get news from database\n",
    "        db_news = self._get_news_from_db(ticker, days_back=365)\n",
    "        \n",
    "        # Combine all data\n",
    "        result = stock_data.copy()\n",
    "        result['scraped_news'] = db_news\n",
    "        result['scraped_news_count'] = len(db_news)\n",
    "        result['new_articles_found'] = new_articles_count\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _get_yfinance_data(self, ticker: str, years_back: int) -> Dict:\n",
    "        \"\"\"Get stock data from yfinance (existing functionality)\"\"\"\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            end_date = datetime.now()\n",
    "            start_date = end_date - timedelta(days=years_back * 365)\n",
    "            \n",
    "            info = stock.info\n",
    "            hist_data = stock.history(start=start_date, end=end_date)\n",
    "            actions = stock.actions\n",
    "            \n",
    "            if not actions.empty:\n",
    "                actions_filtered = actions[actions.index >= start_date.strftime('%Y-%m-%d')]\n",
    "            else:\n",
    "                actions_filtered = pd.DataFrame()\n",
    "            \n",
    "            significant_events = self._identify_significant_events(hist_data)\n",
    "            \n",
    "            return {\n",
    "                'ticker': ticker,\n",
    "                'company_name': info.get('longName', 'N/A'),\n",
    "                'sector': info.get('sector', 'N/A'),\n",
    "                'industry': info.get('industry', 'N/A'),\n",
    "                'date_range': {\n",
    "                    'start': start_date.strftime('%Y-%m-%d'),\n",
    "                    'end': end_date.strftime('%Y-%m-%d')\n",
    "                },\n",
    "                'corporate_actions': self._format_actions(actions_filtered),\n",
    "                'significant_price_events': significant_events,\n",
    "                'summary_stats': {\n",
    "                    'dividends_count': len(actions_filtered[actions_filtered['Dividends'] > 0]) if not actions_filtered.empty else 0,\n",
    "                    'stock_splits_count': len(actions_filtered[actions_filtered['Stock Splits'] > 0]) if not actions_filtered.empty else 0,\n",
    "                    'significant_events_count': len(significant_events)\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting yfinance data: {e}\")\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def _get_news_from_db(self, ticker: str, days_back: int = 365) -> List[Dict]:\n",
    "        \"\"\"Retrieve news articles from database\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cutoff_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        cursor.execute('''\n",
    "            SELECT title, url, content, summary, publisher, author, publish_date, source_site, scraped_date\n",
    "            FROM news_articles \n",
    "            WHERE ticker = ? AND (publish_date >= ? OR publish_date IS NULL)\n",
    "            ORDER BY publish_date DESC, scraped_date DESC\n",
    "        ''', (ticker, cutoff_date))\n",
    "        \n",
    "        rows = cursor.fetchall()\n",
    "        conn.close()\n",
    "        \n",
    "        news_list = []\n",
    "        for row in rows:\n",
    "            news_list.append({\n",
    "                'title': row[0],\n",
    "                'url': row[1],\n",
    "                'content': row[2],\n",
    "                'summary': row[3],\n",
    "                'publisher': row[4],\n",
    "                'author': row[5],\n",
    "                'publish_date': row[6],\n",
    "                'source_site': row[7],\n",
    "                'scraped_date': row[8]\n",
    "            })\n",
    "        \n",
    "        return news_list\n",
    "    \n",
    "    def _format_actions(self, actions: pd.DataFrame) -> List[Dict]:\n",
    "        \"\"\"Format corporate actions data\"\"\"\n",
    "        if actions.empty:\n",
    "            return []\n",
    "        \n",
    "        formatted_actions = []\n",
    "        for date, row in actions.iterrows():\n",
    "            date_str = date.strftime('%Y-%m-%d') if hasattr(date, 'strftime') else str(date)\n",
    "            \n",
    "            if row['Dividends'] > 0:\n",
    "                formatted_actions.append({\n",
    "                    'date': date_str,\n",
    "                    'type': 'Dividend',\n",
    "                    'amount': float(row['Dividends'])\n",
    "                })\n",
    "            if row['Stock Splits'] > 0:\n",
    "                formatted_actions.append({\n",
    "                    'date': date_str,\n",
    "                    'type': 'Stock Split',\n",
    "                    'ratio': float(row['Stock Splits'])\n",
    "                })\n",
    "        \n",
    "        return formatted_actions\n",
    "    \n",
    "    def _identify_significant_events(self, hist_data: pd.DataFrame, threshold: float = 0.05) -> List[Dict]:\n",
    "        \"\"\"Identify significant price movements\"\"\"\n",
    "        if hist_data.empty:\n",
    "            return []\n",
    "        \n",
    "        hist_data['Daily_Return'] = hist_data['Close'].pct_change()\n",
    "        significant_days = hist_data[abs(hist_data['Daily_Return']) > threshold]\n",
    "        \n",
    "        events = []\n",
    "        for date, row in significant_days.iterrows():\n",
    "            date_str = date.strftime('%Y-%m-%d') if hasattr(date, 'strftime') else str(date)\n",
    "            \n",
    "            events.append({\n",
    "                'date': date_str,\n",
    "                'price_change_percent': round(float(row['Daily_Return']) * 100, 2),\n",
    "                'close_price': round(float(row['Close']), 2),\n",
    "                'volume': int(row['Volume']),\n",
    "                'type': 'Significant Price Movement'\n",
    "            })\n",
    "        \n",
    "        events.sort(key=lambda x: x['date'], reverse=True)\n",
    "        return events[:50]\n",
    "    \n",
    "    def print_comprehensive_summary(self, data: Dict):\n",
    "        \"\"\"Print comprehensive summary including scraped news\"\"\"\n",
    "        if 'error' in data:\n",
    "            print(f\"Error: {data['error']}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n=== Comprehensive Summary for {data['ticker']} ({data['company_name']}) ===\")\n",
    "        print(f\"Sector: {data['sector']}\")\n",
    "        print(f\"Industry: {data['industry']}\")\n",
    "        print(f\"Date Range: {data['date_range']['start']} to {data['date_range']['end']}\")\n",
    "        \n",
    "        print(f\"\\n=== News Summary ===\")\n",
    "        print(f\"  • Total Scraped Articles: {data['scraped_news_count']}\")\n",
    "        print(f\"  • New Articles Found: {data['new_articles_found']}\")\n",
    "        print(f\"  • Corporate Actions: {data['summary_stats']['dividends_count']} dividends, {data['summary_stats']['stock_splits_count']} splits\")\n",
    "        print(f\"  • Significant Price Events: {data['summary_stats']['significant_events_count']}\")\n",
    "        \n",
    "        # Show recent scraped news\n",
    "        if data['scraped_news']:\n",
    "            print(f\"\\n=== Recent News (Top 5) ===\")\n",
    "            for i, news in enumerate(data['scraped_news'][:5]):\n",
    "                print(f\"{i+1}. {news['title']}\")\n",
    "                print(f\"   Source: {news['publisher']} ({news['source_site']}) | Date: {news['publish_date']}\")\n",
    "                print(f\"   URL: {news['url']}\")\n",
    "                if news['summary']:\n",
    "                    print(f\"   Summary: {news['summary'][:150]}...\")\n",
    "                print()\n",
    "        \n",
    "        # Setup instructions\n",
    "        print(f\"\\n=== Setup Instructions for Full Functionality ===\")\n",
    "        print(\"To get news from all sources, you'll need API keys (all have free tiers):\")\n",
    "        print(\"1. Alpha Vantage: https://www.alphavantage.co/support/#api-key\")\n",
    "        print(\"2. NewsAPI: https://newsapi.org/\")\n",
    "        print(\"3. Financial Modeling Prep: https://financialmodelingprep.com/\")\n",
    "        print(\"4. Polygon.io: https://polygon.io/\")\n",
    "        print(\"\\nReplace the 'YOUR_*_API_KEY' placeholders in the code with your actual keys.\")\n",
    "    \n",
    "    def export_to_json(self, data: Dict, filename: str = None):\n",
    "        \"\"\"Export data to JSON file\"\"\"\n",
    "        if filename is None:\n",
    "            filename = f\"{data.get('ticker', 'unknown')}_comprehensive_data_{datetime.now().strftime('%Y%m%d')}.json\"\n",
    "        \n",
    "        # Make data JSON serializable\n",
    "        json_data = self._make_json_serializable(data)\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_data, f, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        print(f\"Data exported to {filename}\")\n",
    "    \n",
    "    def _make_json_serializable(self, obj):\n",
    "        \"\"\"Convert objects to JSON serializable format\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: self._make_json_serializable(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._make_json_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, pd.Timestamp):\n",
    "            return obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        elif isinstance(obj, datetime):\n",
    "            return obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        elif hasattr(obj, 'item'):\n",
    "            return obj.item()\n",
    "        elif pd.isna(obj):\n",
    "            return None\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Initialize the enhanced fetcher\n",
    "    fetcher = RobustStockNewsEventsFetcher()\n",
    "    \n",
    "    # Example: Get comprehensive data for Boeing\n",
    "    ticker = \"BA\"\n",
    "    \n",
    "    print(\"=== Robust Stock News and Events Fetcher ===\")\n",
    "    print(\"Features:\")\n",
    "    print(\"  • Multiple API-based news sources (more reliable than web scraping)\")\n",
    "    print(\"  • SQLite database storage with duplicate detection\")\n",
    "    print(\"  • Comprehensive stock event tracking\")\n",
    "    print(\"  • Fallback strategies for when sources fail\")\n",
    "    print(\"\\nRequired packages: pip install yfinance pandas beautifulsoup4 requests\")\n",
    "    print(f\"\\nFetching comprehensive data for {ticker}...\")\n",
    "    \n",
    "    # Get comprehensive data\n",
    "    data = fetcher.get_stock_data_and_news(ticker, years_back=2)\n",
    "    \n",
    "    # Print comprehensive summary\n",
    "    fetcher.print_comprehensive_summary(data)\n",
    "    \n",
    "    # Export to JSON\n",
    "    fetcher.export_to_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ee9557a-9b09-4ccb-9da6-2537579b82d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting comprehensive data collection for BA\n",
      "INFO:__main__:Fetching from Yahoo Finance News API for BA...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded from: private/apikeys.ini\n",
      "=== Robust Stock News and Events Fetcher ===\n",
      "Features:\n",
      "  • Multiple API-based news sources (more reliable than web scraping)\n",
      "  • SQLite database storage with duplicate detection\n",
      "  • Comprehensive stock event tracking\n",
      "  • Fallback strategies for when sources fail\n",
      "\n",
      "Required packages: pip install yfinance pandas beautifulsoup4 requests\n",
      "\n",
      "Fetching comprehensive data for BA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:News already exists: ...\n",
      "INFO:__main__:News already exists: ...\n",
      "INFO:__main__:News already exists: ...\n",
      "INFO:__main__:News already exists: ...\n",
      "INFO:__main__:News already exists: ...\n",
      "INFO:__main__:News already exists: ...\n",
      "INFO:__main__:News already exists: ...\n",
      "INFO:__main__:News already exists: ...\n",
      "INFO:__main__:News already exists: ...\n",
      "INFO:__main__:News already exists: ...\n",
      "INFO:__main__:Found 10 articles from yahoo_finance_api, 0 were new\n",
      "INFO:__main__:Fetching from Alpha Vantage News API for BA...\n",
      "INFO:__main__:Saved new news: INVESTOR ALERT: Pomerantz Law Firm Investigates Cl...\n",
      "INFO:__main__:Saved new news: Wall Street Hits Record Highs, Nike Jumps 18%: Wha...\n",
      "INFO:__main__:Saved new news: RTX Clinches a $250M Deal From MELCO to Produce ES...\n",
      "INFO:__main__:Saved new news: Howmet vs. Textron: Which Aerospace & Defense Stoc...\n",
      "INFO:__main__:Saved new news: Lockheed Secures a $250M Contract Involving F-35 F...\n",
      "INFO:__main__:Saved new news: A CAPITOL FOURTH CELEBRATES 45 YEARS OF MUSIC, FUN...\n",
      "INFO:__main__:Saved new news: Investing in Space: NATO holds out its hand to the...\n",
      "INFO:__main__:Saved new news: Boeing 787 Dreamliner Black Box Recovered, Data Un...\n",
      "INFO:__main__:Saved new news: Why Airbus Stock Popped Thursday...\n",
      "INFO:__main__:Saved new news: Airloom Energy Takes Critical Step for the Future ...\n",
      "INFO:__main__:Found 10 articles from alpha_vantage_news, 10 were new\n",
      "INFO:__main__:Fetching from NewsAPI.org for BA...\n",
      "INFO:__main__:Saved new news: Boeing Wins Buy Rating as Redburn Sees ‘Healthier’...\n",
      "INFO:__main__:Saved new news: Army Drone Research Report 2021-2024 & 2025-2031 |...\n",
      "INFO:__main__:Saved new news: Broker’s call: Unimech Aerospace (Buy)...\n",
      "INFO:__main__:Saved new news: Israel's top ballistic missile shield proved itsel...\n",
      "INFO:__main__:Saved new news: Job openings among the largest U.S. federal contra...\n",
      "INFO:__main__:Saved new news: FedEx Retires 12 Aircraft To Improve Efficiency...\n",
      "INFO:__main__:Saved new news: Defense Stocks To Follow Today – June 24th...\n",
      "INFO:__main__:Saved new news: Airline Stocks To Keep An Eye On – June 24th...\n",
      "INFO:__main__:Saved new news: Artificial Intelligence (AI) in Military Market 20...\n",
      "INFO:__main__:Found 9 articles from newsapi, 9 were new\n",
      "INFO:__main__:Fetching from Financial Modeling Prep for BA...\n",
      "INFO:__main__:Found 0 articles from financial_modeling_prep, 0 were new\n",
      "INFO:__main__:Fetching from Polygon.io News for BA...\n",
      "INFO:__main__:Saved new news: INVESTOR ALERT: Pomerantz Law Firm Investigates Cl...\n",
      "INFO:__main__:News already exists: Wall Street Hits Record Highs, Nike Jumps 18%: Wha...\n",
      "INFO:__main__:Saved new news: BOEING ALERT: Bragar Eagel & Squire, P.C. is Inves...\n",
      "INFO:__main__:Saved new news: INVESTOR ALERT: Pomerantz Law Firm Investigates Cl...\n",
      "INFO:__main__:Saved new news: INVESTOR ALERT: Pomerantz Law Firm Investigates Cl...\n",
      "INFO:__main__:Saved new news: INVESTOR ALERT: Pomerantz Law Firm Investigates Cl...\n",
      "INFO:__main__:Saved new news: Boeing Could Avoid a Trial...\n",
      "INFO:__main__:Saved new news: Defense Drone Research Analysis Report 2025-2034: ...\n",
      "INFO:__main__:Saved new news: Stocks Flat, Dollar Sinks To Over 2-Year Lows Afte...\n",
      "INFO:__main__:Saved new news: Why Boeing Stock Is Under Pressure Today...\n",
      "INFO:__main__:Found 10 articles from polygon_news, 9 were new\n",
      "INFO:__main__:Scraped 28 new articles for BA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comprehensive Summary for BA (The Boeing Company) ===\n",
      "Sector: Industrials\n",
      "Industry: Aerospace & Defense\n",
      "Date Range: 2023-06-29 to 2025-06-28\n",
      "\n",
      "=== News Summary ===\n",
      "  • Total Scraped Articles: 29\n",
      "  • New Articles Found: 28\n",
      "  • Corporate Actions: 0 dividends, 0 splits\n",
      "  • Significant Price Events: 15\n",
      "\n",
      "=== Recent News (Top 5) ===\n",
      "1. INVESTOR ALERT: Pomerantz Law Firm Investigates Claims On Behalf of Investors of The Boeing Company - BA - Boeing  ( NYSE:BA ) \n",
      "   Source: Benzinga (alpha_vantage) | Date: 20250628T140000\n",
      "   URL: https://www.benzinga.com/pressreleases/25/06/g46157463/investor-alert-pomerantz-law-firm-investigates-claims-on-behalf-of-investors-of-the-boeing-company\n",
      "   Summary: NEW YORK, June 28, 2025 ( GLOBE NEWSWIRE ) -- Pomerantz LLP is investigating claims on behalf of investors of The Boeing Company ( \"Boeing\" or the \"Co...\n",
      "\n",
      "2. Wall Street Hits Record Highs, Nike Jumps 18%: What's Moving Markets Friday? Wall Street Hits Record Highs, Nike Jumps 18%: What's Moving Markets Friday? - Boeing  ( NYSE:BA ) \n",
      "   Source: Benzinga (alpha_vantage) | Date: 20250627T171124\n",
      "   URL: https://www.benzinga.com/markets/equities/25/06/46147570/markets-today-news-wall-street-dow-jones-nasdaq-sp500-nike\n",
      "   Summary: S&P 500 and Nasdaq 100 hit new record highs on trade optimism. Nike jumps 18% after earnings beat, leading Dow's charge toward 44,000. Market-moving n...\n",
      "\n",
      "3. RTX Clinches a $250M Deal From MELCO to Produce ESSM Block 2\n",
      "   Source: Zacks Commentary (alpha_vantage) | Date: 20250627T135200\n",
      "   URL: https://www.zacks.com/stock/news/2556036/rtx-clinches-a-250m-deal-from-melco-to-produce-essm-block-2\n",
      "   Summary: RTX strikes a $250M ESSM Block II missile deal with MELCO, boosting Japan's self-reliant defense manufacturing push and expanding global defense reach...\n",
      "\n",
      "4. Howmet vs. Textron: Which Aerospace & Defense Stock has Better Prospects?\n",
      "   Source: Zacks Commentary (alpha_vantage) | Date: 20250627T133300\n",
      "   URL: https://www.zacks.com/stock/news/2556125/howmet-vs-textron-which-aerospace-defense-stock-has-better-prospects\n",
      "   Summary: HWM's soaring earnings growth, strong aerospace demand and shareholder returns set it apart from rival TXT....\n",
      "\n",
      "5. Lockheed Secures a $250M Contract Involving F-35 Fighter Jet Program\n",
      "   Source: Zacks Commentary (alpha_vantage) | Date: 20250627T132600\n",
      "   URL: https://www.zacks.com/stock/news/2556114/lockheed-secures-a-250m-contract-involving-f-35-fighter-jet-program\n",
      "   Summary: Teaser: LMT lands a $250M F-35 logistics systems deal supporting U.S. forces and global program partners through 2027....\n",
      "\n",
      "\n",
      "=== Setup Instructions for Full Functionality ===\n",
      "To get news from all sources, you'll need API keys (all have free tiers):\n",
      "1. Alpha Vantage: https://www.alphavantage.co/support/#api-key\n",
      "2. NewsAPI: https://newsapi.org/\n",
      "3. Financial Modeling Prep: https://financialmodelingprep.com/\n",
      "4. Polygon.io: https://polygon.io/\n",
      "\n",
      "Replace the 'YOUR_*_API_KEY' placeholders in the code with your actual keys.\n",
      "Data exported to BA_comprehensive_data_20250628.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize fetcher\n",
    "fetcher = RobustStockNewsEventsFetcher()\n",
    "    \n",
    "# Example: Get comprehensive data for Boeing\n",
    "ticker = \"BA\"\n",
    "\n",
    "print(\"=== Robust Stock News and Events Fetcher ===\")\n",
    "print(\"Features:\")\n",
    "print(\"  • Multiple API-based news sources (more reliable than web scraping)\")\n",
    "print(\"  • SQLite database storage with duplicate detection\")\n",
    "print(\"  • Comprehensive stock event tracking\")\n",
    "print(\"  • Fallback strategies for when sources fail\")\n",
    "print(\"\\nRequired packages: pip install yfinance pandas beautifulsoup4 requests\")\n",
    "print(f\"\\nFetching comprehensive data for {ticker}...\")\n",
    "\n",
    "# Get comprehensive data\n",
    "data = fetcher.get_stock_data_and_news(ticker, years_back=2)\n",
    "\n",
    "# Print comprehensive summary\n",
    "fetcher.print_comprehensive_summary(data)\n",
    "\n",
    "# Export to JSON\n",
    "fetcher.export_to_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f202ade9-5013-467a-9c24-72ff9abd647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqldata = fetcher._get_news_from_db(ticker=\"BA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d642c056-8ee5-4584-969f-f46bc5b70a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': \"Wall Street Hits Record Highs, Nike Jumps 18%: What's Moving Markets Friday? Wall Street Hits Record Highs, Nike Jumps 18%: What's Moving Markets Friday? - Boeing  ( NYSE:BA ) \",\n",
       " 'url': 'https://www.benzinga.com/markets/equities/25/06/46147570/markets-today-news-wall-street-dow-jones-nasdaq-sp500-nike',\n",
       " 'content': \"S&P 500 and Nasdaq 100 hit new record highs on trade optimism. Nike jumps 18% after earnings beat, leading Dow's charge toward 44,000. Market-moving news hits Benzinga Pro first-get a 30-minute edge and save 60% this 4th of July.\",\n",
       " 'summary': \"S&P 500 and Nasdaq 100 hit new record highs on trade optimism. Nike jumps 18% after earnings beat, leading Dow's charge toward 44,000. Market-moving news hits Benzinga Pro first-get a 30-minute edge a...\",\n",
       " 'publisher': 'Benzinga',\n",
       " 'author': 'Piero Cingari',\n",
       " 'publish_date': '20250627T171124',\n",
       " 'source_site': 'alpha_vantage',\n",
       " 'scraped_date': '2025-06-28 22:06:51'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqldata[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "992b207e-9918-4e43-af14-81f27fb27aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded from: private/apikeys.ini\n"
     ]
    }
   ],
   "source": [
    "# \"UL6SJRQMIVMO4IQR\"\n",
    "\n",
    "from mycongif import ConfigManager\n",
    "apikeys = ConfigManager(\"private/apikeys.ini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b676826-6001-4737-886f-16ceada94f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new section: [polygon]\n",
      "Added/Updated key: [polygon]apikey = vOV6QumNVGPIwq2pjRCf1U0MS0APOFDA\n",
      "Added/Updated key: [polygon]url_apikey = https://polygon.io\n",
      "Configuration saved to: private/apikeys.ini\n"
     ]
    }
   ],
   "source": [
    "apikeys.add_key(\"polygon\", \"apikey\", \"vOV6QumNVGPIwq2pjRCf1U0MS0APOFDA\")\n",
    "apikeys.add_key(\"polygon\", \"url_apikey\", \"https://polygon.io\")\n",
    "apikeys.save_config()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e311",
   "language": "python",
   "name": "e311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
