{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "100ec240-b24b-468a-9825-5071a28f1f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import requests\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AdvancedStockNewsEventsFetcher:\n",
    "    def __init__(self, db_path: str = \"stock_news.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        self.init_database()\n",
    "        \n",
    "        # News sources configuration\n",
    "        self.news_sources = {\n",
    "            'yahoo_finance': {\n",
    "                'base_url': 'https://finance.yahoo.com/quote/{ticker}/news',\n",
    "                'parser': self._parse_yahoo_news\n",
    "            },\n",
    "            'marketwatch': {\n",
    "                'base_url': 'https://www.marketwatch.com/investing/stock/{ticker}',\n",
    "                'parser': self._parse_marketwatch_news\n",
    "            },\n",
    "            'reuters': {\n",
    "                'base_url': 'https://www.reuters.com/markets/companies/{ticker}',\n",
    "                'parser': self._parse_reuters_news\n",
    "            },\n",
    "            'seeking_alpha': {\n",
    "                'base_url': 'https://seekingalpha.com/symbol/{ticker}/news',\n",
    "                'parser': self._parse_seeking_alpha_news\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def init_database(self):\n",
    "        \"\"\"Initialize SQLite database with required tables\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create news table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS news_articles (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                ticker TEXT NOT NULL,\n",
    "                title TEXT NOT NULL,\n",
    "                url TEXT NOT NULL,\n",
    "                content TEXT,\n",
    "                summary TEXT,\n",
    "                publisher TEXT,\n",
    "                author TEXT,\n",
    "                publish_date DATETIME,\n",
    "                scraped_date DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "                source_site TEXT,\n",
    "                content_hash TEXT UNIQUE,\n",
    "                sentiment_score REAL,\n",
    "                UNIQUE(url, ticker)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Create events table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS stock_events (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                ticker TEXT NOT NULL,\n",
    "                event_date DATE,\n",
    "                event_type TEXT,\n",
    "                event_description TEXT,\n",
    "                event_value REAL,\n",
    "                scraped_date DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "                UNIQUE(ticker, event_date, event_type, event_description)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Create price movements table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS price_movements (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                ticker TEXT NOT NULL,\n",
    "                movement_date DATE,\n",
    "                price_change_percent REAL,\n",
    "                close_price REAL,\n",
    "                volume INTEGER,\n",
    "                movement_type TEXT,\n",
    "                scraped_date DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "                UNIQUE(ticker, movement_date)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Create indexes for better performance\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_news_ticker_date ON news_articles(ticker, publish_date)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_events_ticker_date ON stock_events(ticker, event_date)')\n",
    "        cursor.execute('CREATE INDEX IF NOT EXISTS idx_movements_ticker_date ON price_movements(ticker, movement_date)')\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def get_content_hash(self, content: str) -> str:\n",
    "        \"\"\"Generate MD5 hash of content to detect duplicates\"\"\"\n",
    "        return hashlib.md5(content.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    def is_news_exists(self, url: str, ticker: str, content_hash: str) -> bool:\n",
    "        \"\"\"Check if news article already exists in database\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute('''\n",
    "            SELECT id FROM news_articles \n",
    "            WHERE (url = ? AND ticker = ?) OR content_hash = ?\n",
    "        ''', (url, ticker, content_hash))\n",
    "        \n",
    "        exists = cursor.fetchone() is not None\n",
    "        conn.close()\n",
    "        return exists\n",
    "    \n",
    "    def save_news_to_db(self, news_data: Dict, ticker: str):\n",
    "        \"\"\"Save news article to database if it doesn't exist\"\"\"\n",
    "        content_hash = self.get_content_hash(news_data.get('content', '') + news_data.get('title', ''))\n",
    "        \n",
    "        if self.is_news_exists(news_data.get('url', ''), ticker, content_hash):\n",
    "            logger.info(f\"News already exists: {news_data.get('title', 'Unknown')[:50]}...\")\n",
    "            return False\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO news_articles \n",
    "                (ticker, title, url, content, summary, publisher, author, publish_date, source_site, content_hash)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                ticker,\n",
    "                news_data.get('title', ''),\n",
    "                news_data.get('url', ''),\n",
    "                news_data.get('content', ''),\n",
    "                news_data.get('summary', ''),\n",
    "                news_data.get('publisher', ''),\n",
    "                news_data.get('author', ''),\n",
    "                news_data.get('publish_date'),\n",
    "                news_data.get('source_site', ''),\n",
    "                content_hash\n",
    "            ))\n",
    "            conn.commit()\n",
    "            logger.info(f\"Saved new news: {news_data.get('title', 'Unknown')[:50]}...\")\n",
    "            return True\n",
    "        except sqlite3.IntegrityError:\n",
    "            logger.info(f\"Duplicate news detected: {news_data.get('title', 'Unknown')[:50]}...\")\n",
    "            return False\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def scrape_news_for_ticker(self, ticker: str, days_back: int = 30) -> int:\n",
    "        \"\"\"Scrape news from multiple sources for a given ticker\"\"\"\n",
    "        total_new_articles = 0\n",
    "        \n",
    "        for source_name, source_config in self.news_sources.items():\n",
    "            logger.info(f\"Scraping {source_name} for {ticker}...\")\n",
    "            \n",
    "            try:\n",
    "                url = source_config['base_url'].format(ticker=ticker.upper())\n",
    "                response = self.session.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Parse news using source-specific parser\n",
    "                articles = source_config['parser'](response.text, ticker, source_name)\n",
    "                \n",
    "                # Save new articles to database\n",
    "                new_count = 0\n",
    "                for article in articles:\n",
    "                    if self.save_news_to_db(article, ticker):\n",
    "                        new_count += 1\n",
    "                \n",
    "                total_new_articles += new_count\n",
    "                logger.info(f\"Found {len(articles)} articles from {source_name}, {new_count} were new\")\n",
    "                \n",
    "                # Be respectful to servers\n",
    "                time.sleep(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error scraping {source_name} for {ticker}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return total_new_articles\n",
    "    \n",
    "    def _parse_yahoo_news(self, html: str, ticker: str, source: str) -> List[Dict]:\n",
    "        \"\"\"Parse Yahoo Finance news\"\"\"\n",
    "        articles = []\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Look for news items in Yahoo Finance structure\n",
    "        news_items = soup.find_all(['li', 'div'], class_=re.compile(r'js-stream-content|story|article'))\n",
    "        \n",
    "        for item in news_items[:10]:  # Limit to first 10 articles\n",
    "            try:\n",
    "                # Extract title\n",
    "                title_elem = item.find(['h3', 'h4', 'a'], class_=re.compile(r'title|headline'))\n",
    "                if not title_elem:\n",
    "                    title_elem = item.find('a')\n",
    "                \n",
    "                title = title_elem.get_text(strip=True) if title_elem else ''\n",
    "                \n",
    "                # Extract URL\n",
    "                link_elem = title_elem if title_elem and title_elem.name == 'a' else item.find('a')\n",
    "                url = link_elem.get('href', '') if link_elem else ''\n",
    "                if url and not url.startswith('http'):\n",
    "                    url = urljoin('https://finance.yahoo.com', url)\n",
    "                \n",
    "                # Extract summary/content\n",
    "                summary_elem = item.find(['p', 'div'], class_=re.compile(r'summary|content|description'))\n",
    "                summary = summary_elem.get_text(strip=True) if summary_elem else ''\n",
    "                \n",
    "                # Extract date\n",
    "                date_elem = item.find(['time', 'span'], class_=re.compile(r'date|time'))\n",
    "                publish_date = None\n",
    "                if date_elem:\n",
    "                    date_text = date_elem.get_text(strip=True)\n",
    "                    publish_date = self._parse_date(date_text)\n",
    "                \n",
    "                if title and url:\n",
    "                    articles.append({\n",
    "                        'title': title,\n",
    "                        'url': url,\n",
    "                        'content': summary,\n",
    "                        'summary': summary[:200] + '...' if len(summary) > 200 else summary,\n",
    "                        'publisher': 'Yahoo Finance',\n",
    "                        'author': '',\n",
    "                        'publish_date': publish_date,\n",
    "                        'source_site': source\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Error parsing Yahoo news item: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    def _parse_marketwatch_news(self, html: str, ticker: str, source: str) -> List[Dict]:\n",
    "        \"\"\"Parse MarketWatch news\"\"\"\n",
    "        articles = []\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Look for news items in MarketWatch structure\n",
    "        news_items = soup.find_all(['div', 'article'], class_=re.compile(r'article|story|news'))\n",
    "        \n",
    "        for item in news_items[:10]:\n",
    "            try:\n",
    "                # Extract title\n",
    "                title_elem = item.find(['h2', 'h3', 'h4'], class_=re.compile(r'headline|title'))\n",
    "                if not title_elem:\n",
    "                    title_elem = item.find('a')\n",
    "                \n",
    "                title = title_elem.get_text(strip=True) if title_elem else ''\n",
    "                \n",
    "                # Extract URL\n",
    "                link_elem = title_elem.find('a') if title_elem else item.find('a')\n",
    "                url = link_elem.get('href', '') if link_elem else ''\n",
    "                if url and not url.startswith('http'):\n",
    "                    url = urljoin('https://www.marketwatch.com', url)\n",
    "                \n",
    "                # Extract summary\n",
    "                summary_elem = item.find(['p', 'div'], class_=re.compile(r'summary|excerpt'))\n",
    "                summary = summary_elem.get_text(strip=True) if summary_elem else ''\n",
    "                \n",
    "                if title and url:\n",
    "                    articles.append({\n",
    "                        'title': title,\n",
    "                        'url': url,\n",
    "                        'content': summary,\n",
    "                        'summary': summary[:200] + '...' if len(summary) > 200 else summary,\n",
    "                        'publisher': 'MarketWatch',\n",
    "                        'author': '',\n",
    "                        'publish_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "                        'source_site': source\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Error parsing MarketWatch news item: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    def _parse_reuters_news(self, html: str, ticker: str, source: str) -> List[Dict]:\n",
    "        \"\"\"Parse Reuters news\"\"\"\n",
    "        articles = []\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Look for news items in Reuters structure\n",
    "        news_items = soup.find_all(['div', 'article'], class_=re.compile(r'story|article|news'))\n",
    "        \n",
    "        for item in news_items[:10]:\n",
    "            try:\n",
    "                title_elem = item.find(['h3', 'h4'], class_=re.compile(r'headline|title'))\n",
    "                title = title_elem.get_text(strip=True) if title_elem else ''\n",
    "                \n",
    "                link_elem = item.find('a')\n",
    "                url = link_elem.get('href', '') if link_elem else ''\n",
    "                if url and not url.startswith('http'):\n",
    "                    url = urljoin('https://www.reuters.com', url)\n",
    "                \n",
    "                if title and url:\n",
    "                    articles.append({\n",
    "                        'title': title,\n",
    "                        'url': url,\n",
    "                        'content': '',\n",
    "                        'summary': title,\n",
    "                        'publisher': 'Reuters',\n",
    "                        'author': '',\n",
    "                        'publish_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "                        'source_site': source\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Error parsing Reuters news item: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    def _parse_seeking_alpha_news(self, html: str, ticker: str, source: str) -> List[Dict]:\n",
    "        \"\"\"Parse Seeking Alpha news\"\"\"\n",
    "        articles = []\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Look for news items in Seeking Alpha structure\n",
    "        news_items = soup.find_all(['article', 'div'], class_=re.compile(r'article|post|story'))\n",
    "        \n",
    "        for item in news_items[:10]:\n",
    "            try:\n",
    "                title_elem = item.find(['h2', 'h3'], class_=re.compile(r'title|headline'))\n",
    "                title = title_elem.get_text(strip=True) if title_elem else ''\n",
    "                \n",
    "                link_elem = item.find('a')\n",
    "                url = link_elem.get('href', '') if link_elem else ''\n",
    "                if url and not url.startswith('http'):\n",
    "                    url = urljoin('https://seekingalpha.com', url)\n",
    "                \n",
    "                if title and url:\n",
    "                    articles.append({\n",
    "                        'title': title,\n",
    "                        'url': url,\n",
    "                        'content': '',\n",
    "                        'summary': title,\n",
    "                        'publisher': 'Seeking Alpha',\n",
    "                        'author': '',\n",
    "                        'publish_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "                        'source_site': source\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Error parsing Seeking Alpha news item: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    def _parse_date(self, date_text: str) -> str:\n",
    "        \"\"\"Parse various date formats to standard format\"\"\"\n",
    "        try:\n",
    "            # Handle relative dates like \"2 hours ago\", \"1 day ago\"\n",
    "            if 'ago' in date_text.lower():\n",
    "                if 'hour' in date_text or 'minute' in date_text:\n",
    "                    return datetime.now().strftime('%Y-%m-%d')\n",
    "                elif 'day' in date_text:\n",
    "                    days = re.findall(r'\\d+', date_text)\n",
    "                    if days:\n",
    "                        date = datetime.now() - timedelta(days=int(days[0]))\n",
    "                        return date.strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Try to parse standard date formats\n",
    "            date = pd.to_datetime(date_text)\n",
    "            return date.strftime('%Y-%m-%d')\n",
    "        except:\n",
    "            return datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    def get_stock_data_and_news(self, ticker: str, years_back: int = 10) -> Dict:\n",
    "        \"\"\"Get comprehensive stock data including scraped news\"\"\"\n",
    "        logger.info(f\"Starting comprehensive data collection for {ticker}\")\n",
    "        \n",
    "        # First, scrape fresh news\n",
    "        new_articles_count = self.scrape_news_for_ticker(ticker, days_back=365)\n",
    "        logger.info(f\"Scraped {new_articles_count} new articles for {ticker}\")\n",
    "        \n",
    "        # Get traditional yfinance data\n",
    "        stock_data = self._get_yfinance_data(ticker, years_back)\n",
    "        \n",
    "        # Get news from database\n",
    "        db_news = self._get_news_from_db(ticker, days_back=365)\n",
    "        \n",
    "        # Combine all data\n",
    "        result = stock_data.copy()\n",
    "        result['scraped_news'] = db_news\n",
    "        result['scraped_news_count'] = len(db_news)\n",
    "        result['new_articles_found'] = new_articles_count\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _get_yfinance_data(self, ticker: str, years_back: int) -> Dict:\n",
    "        \"\"\"Get stock data from yfinance (existing functionality)\"\"\"\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            end_date = datetime.now()\n",
    "            start_date = end_date - timedelta(days=years_back * 365)\n",
    "            \n",
    "            info = stock.info\n",
    "            hist_data = stock.history(start=start_date, end=end_date)\n",
    "            actions = stock.actions\n",
    "            \n",
    "            if not actions.empty:\n",
    "                actions_filtered = actions[actions.index >= start_date.strftime('%Y-%m-%d')]\n",
    "            else:\n",
    "                actions_filtered = pd.DataFrame()\n",
    "            \n",
    "            significant_events = self._identify_significant_events(hist_data)\n",
    "            \n",
    "            return {\n",
    "                'ticker': ticker,\n",
    "                'company_name': info.get('longName', 'N/A'),\n",
    "                'sector': info.get('sector', 'N/A'),\n",
    "                'industry': info.get('industry', 'N/A'),\n",
    "                'date_range': {\n",
    "                    'start': start_date.strftime('%Y-%m-%d'),\n",
    "                    'end': end_date.strftime('%Y-%m-%d')\n",
    "                },\n",
    "                'corporate_actions': self._format_actions(actions_filtered),\n",
    "                'significant_price_events': significant_events,\n",
    "                'summary_stats': {\n",
    "                    'dividends_count': len(actions_filtered[actions_filtered['Dividends'] > 0]) if not actions_filtered.empty else 0,\n",
    "                    'stock_splits_count': len(actions_filtered[actions_filtered['Stock Splits'] > 0]) if not actions_filtered.empty else 0,\n",
    "                    'significant_events_count': len(significant_events)\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting yfinance data: {e}\")\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def _get_news_from_db(self, ticker: str, days_back: int = 365) -> List[Dict]:\n",
    "        \"\"\"Retrieve news articles from database\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cutoff_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        cursor.execute('''\n",
    "            SELECT title, url, content, summary, publisher, author, publish_date, source_site, scraped_date\n",
    "            FROM news_articles \n",
    "            WHERE ticker = ? AND (publish_date >= ? OR publish_date IS NULL)\n",
    "            ORDER BY publish_date DESC, scraped_date DESC\n",
    "        ''', (ticker, cutoff_date))\n",
    "        \n",
    "        rows = cursor.fetchall()\n",
    "        conn.close()\n",
    "        \n",
    "        news_list = []\n",
    "        for row in rows:\n",
    "            news_list.append({\n",
    "                'title': row[0],\n",
    "                'url': row[1],\n",
    "                'content': row[2],\n",
    "                'summary': row[3],\n",
    "                'publisher': row[4],\n",
    "                'author': row[5],\n",
    "                'publish_date': row[6],\n",
    "                'source_site': row[7],\n",
    "                'scraped_date': row[8]\n",
    "            })\n",
    "        \n",
    "        return news_list\n",
    "    \n",
    "    def _format_actions(self, actions: pd.DataFrame) -> List[Dict]:\n",
    "        \"\"\"Format corporate actions data\"\"\"\n",
    "        if actions.empty:\n",
    "            return []\n",
    "        \n",
    "        formatted_actions = []\n",
    "        for date, row in actions.iterrows():\n",
    "            date_str = date.strftime('%Y-%m-%d') if hasattr(date, 'strftime') else str(date)\n",
    "            \n",
    "            if row['Dividends'] > 0:\n",
    "                formatted_actions.append({\n",
    "                    'date': date_str,\n",
    "                    'type': 'Dividend',\n",
    "                    'amount': float(row['Dividends'])\n",
    "                })\n",
    "            if row['Stock Splits'] > 0:\n",
    "                formatted_actions.append({\n",
    "                    'date': date_str,\n",
    "                    'type': 'Stock Split',\n",
    "                    'ratio': float(row['Stock Splits'])\n",
    "                })\n",
    "        \n",
    "        return formatted_actions\n",
    "    \n",
    "    def _identify_significant_events(self, hist_data: pd.DataFrame, threshold: float = 0.05) -> List[Dict]:\n",
    "        \"\"\"Identify significant price movements\"\"\"\n",
    "        if hist_data.empty:\n",
    "            return []\n",
    "        \n",
    "        hist_data['Daily_Return'] = hist_data['Close'].pct_change()\n",
    "        significant_days = hist_data[abs(hist_data['Daily_Return']) > threshold]\n",
    "        \n",
    "        events = []\n",
    "        for date, row in significant_days.iterrows():\n",
    "            date_str = date.strftime('%Y-%m-%d') if hasattr(date, 'strftime') else str(date)\n",
    "            \n",
    "            events.append({\n",
    "                'date': date_str,\n",
    "                'price_change_percent': round(float(row['Daily_Return']) * 100, 2),\n",
    "                'close_price': round(float(row['Close']), 2),\n",
    "                'volume': int(row['Volume']),\n",
    "                'type': 'Significant Price Movement'\n",
    "            })\n",
    "        \n",
    "        events.sort(key=lambda x: x['date'], reverse=True)\n",
    "        return events[:50]\n",
    "    \n",
    "    def print_comprehensive_summary(self, data: Dict):\n",
    "        \"\"\"Print comprehensive summary including scraped news\"\"\"\n",
    "        if 'error' in data:\n",
    "            print(f\"Error: {data['error']}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n=== Comprehensive Summary for {data['ticker']} ({data['company_name']}) ===\")\n",
    "        print(f\"Sector: {data['sector']}\")\n",
    "        print(f\"Industry: {data['industry']}\")\n",
    "        print(f\"Date Range: {data['date_range']['start']} to {data['date_range']['end']}\")\n",
    "        \n",
    "        print(f\"\\n=== News Summary ===\")\n",
    "        print(f\"  • Total Scraped Articles: {data['scraped_news_count']}\")\n",
    "        print(f\"  • New Articles Found: {data['new_articles_found']}\")\n",
    "        print(f\"  • Corporate Actions: {data['summary_stats']['dividends_count']} dividends, {data['summary_stats']['stock_splits_count']} splits\")\n",
    "        print(f\"  • Significant Price Events: {data['summary_stats']['significant_events_count']}\")\n",
    "        \n",
    "        # Show recent scraped news\n",
    "        if data['scraped_news']:\n",
    "            print(f\"\\n=== Recent Scraped News (Top 5) ===\")\n",
    "            for i, news in enumerate(data['scraped_news'][:5]):\n",
    "                print(f\"{i+1}. {news['title']}\")\n",
    "                print(f\"   Source: {news['publisher']} ({news['source_site']}) | Date: {news['publish_date']}\")\n",
    "                print(f\"   URL: {news['url']}\")\n",
    "                if news['summary']:\n",
    "                    print(f\"   Summary: {news['summary'][:150]}...\")\n",
    "                print()\n",
    "    \n",
    "    def export_to_json(self, data: Dict, filename: str = None):\n",
    "        \"\"\"Export data to JSON file\"\"\"\n",
    "        if filename is None:\n",
    "            filename = f\"{data.get('ticker', 'unknown')}_comprehensive_data_{datetime.now().strftime('%Y%m%d')}.json\"\n",
    "        \n",
    "        # Make data JSON serializable\n",
    "        json_data = self._make_json_serializable(data)\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_data, f, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "        print(f\"Data exported to {filename}\")\n",
    "    \n",
    "    def _make_json_serializable(self, obj):\n",
    "        \"\"\"Convert objects to JSON serializable format\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: self._make_json_serializable(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._make_json_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, pd.Timestamp):\n",
    "            return obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        elif isinstance(obj, datetime):\n",
    "            return obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        elif hasattr(obj, 'item'):\n",
    "            return obj.item()\n",
    "        elif pd.isna(obj):\n",
    "            return None\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Initialize the enhanced fetcher\n",
    "    fetcher = AdvancedStockNewsEventsFetcher()\n",
    "    \n",
    "    # Example: Get comprehensive data for Apple\n",
    "    ticker = \"AAPL\"\n",
    "    \n",
    "    print(\"=== Enhanced Stock News and Events Fetcher ===\")\n",
    "    print(\"Features:\")\n",
    "    print(\"  • Web scraping from multiple financial news sources\")\n",
    "    print(\"  • SQLite database storage with duplicate detection\")\n",
    "    print(\"  • Comprehensive stock event tracking\")\n",
    "    print(\"\\nRequired packages: pip install yfinance pandas beautifulsoup4 requests\")\n",
    "    print(f\"\\nFetching comprehensive data for {ticker}...\")\n",
    "    \n",
    "    # Get comprehensive data (includes web scraping and database storage)\n",
    "    data = fetcher.get_stock_data_and_news(ticker, years_back=2)\n",
    "    \n",
    "    # Print comprehensive summary\n",
    "    fetcher.print_comprehensive_summary(data)\n",
    "    \n",
    "    # Export to JSON\n",
    "    fetcher.export_to_json(data)\n",
    "    \n",
    "    print(f\"\\n=== Database Status ===\")\n",
    "    print(f\"Database location: {fetcher.db_path}\")\n",
    "    print(\"You can query the database directly using SQLite tools or pandas:\")\n",
    "    print(\"  conn = sqlite3.connect('stock_news.db')\")\n",
    "    print(\"  df = pd.read_sql('SELECT * FROM news_articles WHERE ticker = \\\"AAPL\\\"', conn)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee9557a-9b09-4ccb-9da6-2537579b82d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting comprehensive data collection for BA\n",
      "INFO:__main__:Scraping yahoo_finance for BA...\n",
      "ERROR:__main__:Error scraping yahoo_finance for BA: 404 Client Error: Not Found for url: https://finance.yahoo.com/quote/BA/news/\n",
      "INFO:__main__:Scraping marketwatch for BA...\n",
      "ERROR:__main__:Error scraping marketwatch for BA: 401 Client Error: HTTP Forbidden for url: https://www.marketwatch.com/investing/stock/BA\n",
      "INFO:__main__:Scraping reuters for BA...\n",
      "ERROR:__main__:Error scraping reuters for BA: 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/markets/companies/BA\n",
      "INFO:__main__:Scraping seeking_alpha for BA...\n",
      "ERROR:__main__:Error scraping seeking_alpha for BA: 403 Client Error: Forbidden for url: https://seekingalpha.com/symbol/BA/news\n",
      "INFO:__main__:Scraped 0 new articles for BA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comprehensive Summary for BA (The Boeing Company) ===\n",
      "Sector: Industrials\n",
      "Industry: Aerospace & Defense\n",
      "Date Range: 2023-06-29 to 2025-06-28\n",
      "\n",
      "=== News Summary ===\n",
      "  • Total Scraped Articles: 0\n",
      "  • New Articles Found: 0\n",
      "  • Corporate Actions: 0 dividends, 0 splits\n",
      "  • Significant Price Events: 15\n",
      "Data exported to BA_comprehensive_data_20250628.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize fetcher\n",
    "fetcher = AdvancedStockNewsEventsFetcher()\n",
    "\n",
    "# Get comprehensive data (scrapes new news + gets historical data)\n",
    "data = fetcher.get_stock_data_and_news(\"BA\", years_back=2)\n",
    "\n",
    "# Print summary\n",
    "fetcher.print_comprehensive_summary(data)\n",
    "\n",
    "# Export to JSON\n",
    "fetcher.export_to_json(data)\n",
    "\n",
    "# Query database directly\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('stock_news.db')\n",
    "df = pd.read_sql('SELECT * FROM news_articles WHERE ticker = \"AAPL\"', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b207e-9918-4e43-af14-81f27fb27aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e311",
   "language": "python",
   "name": "e311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
